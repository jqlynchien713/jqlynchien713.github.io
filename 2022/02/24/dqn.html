<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>DQN å­¸ç¿’ç­†è¨˜ | Jacquelynâ€™s Blog</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="DQN å­¸ç¿’ç­†è¨˜" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="é€™æ˜¯è«–æ–‡è¦ç ”ç©¶ã„‰ä¸»é¡Œï¼Œæƒ³èªªéƒ½å¯«å¥½äº†é‚£å°±å…ˆæ”¾ä¸Šä¾†å¥½äº† Basic Components Env, Actor, Reward Funciton, Episode &amp; Trajectory Trajectory: \(\tau = \{s_i, a_i,\dots\}\) in 1 episode å¯ä»¥è¨ˆç®—ç‰¹å®š (action, state) pair å‡ºç¾çš„æ©Ÿç‡\(P_\theta(\tau) = p(s_1)p_\theta(a_1\|s_1)p(s_2\|s_1, a_1)p_\theta(a_2\|s_2)p(s_3\|s_2, a_2)\dots\\=P(s_1)\prod_{t=1}^{T}p_\theta(a_t\|s_t)p(s_{t+1}\|s_t, a_t)\) æ›´æ–° \(\theta\) æ‰¾åˆ°æœ€å¤§çš„ reward: \(\max R(\tau)=\sum_t^Tr_t\) Env &amp; Reward ç‚ºéš¨æ©Ÿè®Šæ•¸ï¼Œå› æ­¤åªèƒ½ç®—æœŸæœ›å€¼ï¼š \(\bar{R}_\theta = \sum R(\tau)p_\theta(\tau) = E_{\tau\sim p_\theta(\tau)}[R(\tau)]\) Gradient Ascent: Maximize expected reward ç›®æ¨™ï¼šå¦‚æœæŸå€‹ (state, action) pair èƒ½è®“ trajectory reward ç‚ºæ­£ï¼Œå‰‡å¢åŠ é€™é …å‡ºç¾çš„æ©Ÿç‡ \(\nabla \bar{R_\theta}=\frac{1}{N}\sum_n^N\sum_t^{T_n} R(\tau^n)\nabla\log p_\theta(a_t^n\|s_t^n)\) éœ€è¦ä¸æ–·å¾ trajectory set sample data Tips for implementation å°‡ state(input) &amp; action(output) è¦–ç‚ºåˆ†é¡å•é¡Œ ä¸€èˆ¬åˆ†é¡å•é¡Œ objective functionï¼šmin cross entropy = max log likelihood è½‰æ›æˆ RL æ™‚è¦å¤šä¹˜ä¸Š reward \(\frac{1}{N}\sum_n^N\sum_t^{T_n} \nabla\log p_\theta(a_t^n\|s_t^n)\Rightarrow\frac{1}{N}\sum_n^N\sum_t^{T_n} R(\tau^n)\nabla\log p_\theta(a_t^n\|s_t^n)\) Add a baseline æƒ…å¢ƒï¼šç•¶éŠæˆ²è¦å‰‡è£¡ä¸æœƒå¾—åˆ°è² åˆ†æ™‚ï¼›ä¸”æœ‰äº› action ä¸æœƒè¢« sample åˆ°ï¼Œå°è‡´ agent ä½ä¼°é€™å€‹action èƒ½å¸¶ä¾†çš„ reward è§£æ³•ï¼šè®“ reward ä¸è¦ç¸½æ˜¯æ­£çš„ \(\rightarrow\) Reward - baseline(ex. \(\bar{\tau^n}\)) Assign Suitable Credit å•é¡Œï¼šå°±ç®—æŸå€‹ pair çš„è¡¨ç¾æ˜¯ä¸å¥½çš„ï¼Œä½†åœ¨ç¸½ reward ç‚ºæ­£çš„æƒ…æ³ä¸‹ï¼Œé‚„æ˜¯æœƒè¢«æå‡å‡ºç¾çš„æ©Ÿç‡ è§£æ³•ï¼šåªè¨ˆç®—åŸ·è¡Œæ­¤ action å¾Œçš„ reward æ–°çš„æ›´æ–°æ–¹å¼ï¼š \(\frac{1}{N}\sum_n^N\sum_t^{T_n} (R(\tau^n)-b)\nabla\log p_\theta(a_t^n\|s_t^n)\) b å–æ±ºæ–¼ state \((R(\tau^n)-b)\) ç‚º advantage function \(A^\theta(s_t, a_t)\) Proximal Policy Optimization, PPO Off-Policy On-policy vs. Off-policy On: agent å’Œç’°å¢ƒäº’å‹•ï¼‹å­¸ç¿’ Off: å­¸ç¿’çš„ agent \(\neq\) äº’å‹•çš„ agentï¼Œaka agent åœ¨æ—é‚Šçœ‹åˆ¥äººç©ï¼ˆä¸¦å­¸ï¼‰ On-policy çš„å•é¡Œï¼šæ›´æ–° model å¾Œéœ€è¦é‡æ–° sample data Off-policy å¦‚ä½•è§£æ±ºï¼šsample \(\pi_{\theta&#39;}\) ä¾†è¨“ç·´ \(\pi_\theta \rightarrow\) reuse sampled data Importance Sampling ç„¡æ³•ç›´æ¥å¾ p(x) æŠ½æ¨£çš„æƒ…æ³ä¸‹ï¼Œå¯è—‰ç”±å¦ä¸€å€‹åˆ†éƒ¨ q(x) æŠ½æ¨£ ä¸” sample q æ¬¡æ•¸å¤ å¤šå‰‡å¯è¶Šè¶¨è¿‘æ–¼ p(x) \(E_{x\sim p}[f(x)]=E_{x\sim q}[f(x)\frac{p(x)}{q(x)}]\) \(\frac{p(x)}{q(x)}\)ç‚ºä¿®æ­£å¾ q æŠ½æ¨£çš„èª¿æ•´é … Off-policy åš gradient ascent æ™‚çš„èª¿æ•´ \(\nabla \bar{R_\theta}=E_{\tau\sim p_\theta(\tau)}[R(\tau)\nabla\log p_\theta(\tau)]\rightarrow E_{\tau\sim p_{\theta&#39;}(\tau)}[\frac{p_\theta(\tau)}{p_{\theta&#39;}(\tau)}R(\tau)\nabla\log p_\theta(\tau)]\) New Objective Function \(J^{\theta&#39;}(\theta)=E_{(s_t,a_t)\sim \pi_{\theta&#39;}(\tau)}[\frac{p_\theta(a_t\|s_t)}{p_{\theta&#39;}(a_t\|s_t)}A^{\theta&#39;}(s_t, a_t)]\) \(\theta&#39;\): demo \(\theta\): update PPO: é™åˆ¶ \(\pi\) èˆ‡ \(\pi&#39;\) é–“çš„ç›¸ä¼¼åº¦ \[J_{PPO}^{\theta&#39;}=J^{\theta&#39;}(\theta)-\beta\cdot KL(\theta, \theta&#39;)\] PPO2: Q Learning(Value-based) Learn Critic: è©•åƒ¹ actor \(\pi\) åšå¾—å¤šå¥½ï¼ˆè€Œä¸æ˜¯ stateï¼‰ State value function \(V^\pi(s)\) å®šç¾©ï¼šgiven sï¼Œåˆ°çµæŸçš„ç´¯ç©æœŸæœ› reward Estimate \(V^\pi(s)\) Monte-Carlo based çœ‹åˆ° state a å›å‚³ cumulated gain a ä¸å¯èƒ½æƒéæ‰€æœ‰ state \(\rightarrow V^\pi(s)\) è¦–ç‚º regression Temporal-difference appoarch \(\because s_t, a_t, r_t \rightarrow s_{t+1}, a_{t+1}, r_{t+1}\) MC vs. TD: ä¸åŒæ‰‹æ®µä¼°å‡ºä¾†çš„çµæœå¯èƒ½ä¸åŒ MC: larger varianceï¼Œå› ç‚ºæ¯æ¬¡å¾—åˆ°çš„ state &amp; gain æœ‰æŠ½æ¨£çš„éš¨æ©Ÿæ€§ï¼Œä¸”æ˜¯è¨ˆç®—ç´¯ç© reward TD: smaller varianceï¼Œå› ç‚ºæ¯æ¬¡åªè¨ˆç®—ä¸€å€‹ rï¼Œä½† \(V^\pi\) å¯èƒ½ä¼°çš„ä¸æº– State-action value function \(Q^\pi(s, a)\) å®šç¾©ï¼šåœ¨ state s å¼·åˆ¶æ¡å– action aï¼Œå¾ŒçºŒæ¡ç”¨ \(\pi\) çš„ç´¯ç©æœŸæœ› reward ï¼ˆagent ä¸ä¸€å®šæœƒæ¡ç”¨ aï¼‰ ä½¿ç”¨ Q function æ›´æ–° \(\pi\) çš„æµç¨‹ Tips: Target Network Exploration åŸå› ï¼šQ function æœ‰é»é¡ä¼¼ regressionï¼ˆç„¡éš¨æ©Ÿæ€§ï¼‰ï¼Œä¸é©åˆç”¨æ–¼ sample data è§£æ³•ï¼š Epsilon Greedy: Boltzmann Exploration Replay Buffer æ¸›å°‘å’Œç’°å¢ƒäº’å‹•çš„æ¬¡æ•¸ å¯æå‡ batch data diversity Off-policy: å› ç‚ºéå»çš„ exp ä¸ä¸€å®šä¾†è‡ª \(\pi\) Advanced Tips Double DQN å•é¡Œï¼šDQN æœƒé«˜ä¼° Q value ä¼°è¨ˆèª¤å·® \(\rightarrow\) é¸åˆ°é«˜ä¼°çš„ action \(\rightarrow\) targetå€¼æœƒè¢«è¨­å¤ªé«˜ è§£æ³•ï¼šé¸ action çš„ Q function \(\neq\) ç®— Q value çš„ Q function Dueling DQN æ”¹è®Š network æ¶æ§‹ï¼Œ\(Q(s, a)\rightarrow Q(s, a)=V(s)+A(s, a)\) æå‡ä¼°è¨ˆ Q value çš„æ•ˆç‡ï¼Œä¸ç”¨æ¯å€‹ pair éƒ½è¢« sample \(\because\) æ›´æ–° \(V(s)\) æ¯” sample æœ‰æ•ˆç‡ A çš„ column åˆç‚º 0ï¼Œç¢ºä¿æ›´æ–° V Priority Reply å¾ buffer è£¡é¸æ“‡ TD error è¼ƒå¤§çš„ experience set æ”¹è®Š sample data çš„ distribution &amp; training process" />
<meta property="og:description" content="é€™æ˜¯è«–æ–‡è¦ç ”ç©¶ã„‰ä¸»é¡Œï¼Œæƒ³èªªéƒ½å¯«å¥½äº†é‚£å°±å…ˆæ”¾ä¸Šä¾†å¥½äº† Basic Components Env, Actor, Reward Funciton, Episode &amp; Trajectory Trajectory: \(\tau = \{s_i, a_i,\dots\}\) in 1 episode å¯ä»¥è¨ˆç®—ç‰¹å®š (action, state) pair å‡ºç¾çš„æ©Ÿç‡\(P_\theta(\tau) = p(s_1)p_\theta(a_1\|s_1)p(s_2\|s_1, a_1)p_\theta(a_2\|s_2)p(s_3\|s_2, a_2)\dots\\=P(s_1)\prod_{t=1}^{T}p_\theta(a_t\|s_t)p(s_{t+1}\|s_t, a_t)\) æ›´æ–° \(\theta\) æ‰¾åˆ°æœ€å¤§çš„ reward: \(\max R(\tau)=\sum_t^Tr_t\) Env &amp; Reward ç‚ºéš¨æ©Ÿè®Šæ•¸ï¼Œå› æ­¤åªèƒ½ç®—æœŸæœ›å€¼ï¼š \(\bar{R}_\theta = \sum R(\tau)p_\theta(\tau) = E_{\tau\sim p_\theta(\tau)}[R(\tau)]\) Gradient Ascent: Maximize expected reward ç›®æ¨™ï¼šå¦‚æœæŸå€‹ (state, action) pair èƒ½è®“ trajectory reward ç‚ºæ­£ï¼Œå‰‡å¢åŠ é€™é …å‡ºç¾çš„æ©Ÿç‡ \(\nabla \bar{R_\theta}=\frac{1}{N}\sum_n^N\sum_t^{T_n} R(\tau^n)\nabla\log p_\theta(a_t^n\|s_t^n)\) éœ€è¦ä¸æ–·å¾ trajectory set sample data Tips for implementation å°‡ state(input) &amp; action(output) è¦–ç‚ºåˆ†é¡å•é¡Œ ä¸€èˆ¬åˆ†é¡å•é¡Œ objective functionï¼šmin cross entropy = max log likelihood è½‰æ›æˆ RL æ™‚è¦å¤šä¹˜ä¸Š reward \(\frac{1}{N}\sum_n^N\sum_t^{T_n} \nabla\log p_\theta(a_t^n\|s_t^n)\Rightarrow\frac{1}{N}\sum_n^N\sum_t^{T_n} R(\tau^n)\nabla\log p_\theta(a_t^n\|s_t^n)\) Add a baseline æƒ…å¢ƒï¼šç•¶éŠæˆ²è¦å‰‡è£¡ä¸æœƒå¾—åˆ°è² åˆ†æ™‚ï¼›ä¸”æœ‰äº› action ä¸æœƒè¢« sample åˆ°ï¼Œå°è‡´ agent ä½ä¼°é€™å€‹action èƒ½å¸¶ä¾†çš„ reward è§£æ³•ï¼šè®“ reward ä¸è¦ç¸½æ˜¯æ­£çš„ \(\rightarrow\) Reward - baseline(ex. \(\bar{\tau^n}\)) Assign Suitable Credit å•é¡Œï¼šå°±ç®—æŸå€‹ pair çš„è¡¨ç¾æ˜¯ä¸å¥½çš„ï¼Œä½†åœ¨ç¸½ reward ç‚ºæ­£çš„æƒ…æ³ä¸‹ï¼Œé‚„æ˜¯æœƒè¢«æå‡å‡ºç¾çš„æ©Ÿç‡ è§£æ³•ï¼šåªè¨ˆç®—åŸ·è¡Œæ­¤ action å¾Œçš„ reward æ–°çš„æ›´æ–°æ–¹å¼ï¼š \(\frac{1}{N}\sum_n^N\sum_t^{T_n} (R(\tau^n)-b)\nabla\log p_\theta(a_t^n\|s_t^n)\) b å–æ±ºæ–¼ state \((R(\tau^n)-b)\) ç‚º advantage function \(A^\theta(s_t, a_t)\) Proximal Policy Optimization, PPO Off-Policy On-policy vs. Off-policy On: agent å’Œç’°å¢ƒäº’å‹•ï¼‹å­¸ç¿’ Off: å­¸ç¿’çš„ agent \(\neq\) äº’å‹•çš„ agentï¼Œaka agent åœ¨æ—é‚Šçœ‹åˆ¥äººç©ï¼ˆä¸¦å­¸ï¼‰ On-policy çš„å•é¡Œï¼šæ›´æ–° model å¾Œéœ€è¦é‡æ–° sample data Off-policy å¦‚ä½•è§£æ±ºï¼šsample \(\pi_{\theta&#39;}\) ä¾†è¨“ç·´ \(\pi_\theta \rightarrow\) reuse sampled data Importance Sampling ç„¡æ³•ç›´æ¥å¾ p(x) æŠ½æ¨£çš„æƒ…æ³ä¸‹ï¼Œå¯è—‰ç”±å¦ä¸€å€‹åˆ†éƒ¨ q(x) æŠ½æ¨£ ä¸” sample q æ¬¡æ•¸å¤ å¤šå‰‡å¯è¶Šè¶¨è¿‘æ–¼ p(x) \(E_{x\sim p}[f(x)]=E_{x\sim q}[f(x)\frac{p(x)}{q(x)}]\) \(\frac{p(x)}{q(x)}\)ç‚ºä¿®æ­£å¾ q æŠ½æ¨£çš„èª¿æ•´é … Off-policy åš gradient ascent æ™‚çš„èª¿æ•´ \(\nabla \bar{R_\theta}=E_{\tau\sim p_\theta(\tau)}[R(\tau)\nabla\log p_\theta(\tau)]\rightarrow E_{\tau\sim p_{\theta&#39;}(\tau)}[\frac{p_\theta(\tau)}{p_{\theta&#39;}(\tau)}R(\tau)\nabla\log p_\theta(\tau)]\) New Objective Function \(J^{\theta&#39;}(\theta)=E_{(s_t,a_t)\sim \pi_{\theta&#39;}(\tau)}[\frac{p_\theta(a_t\|s_t)}{p_{\theta&#39;}(a_t\|s_t)}A^{\theta&#39;}(s_t, a_t)]\) \(\theta&#39;\): demo \(\theta\): update PPO: é™åˆ¶ \(\pi\) èˆ‡ \(\pi&#39;\) é–“çš„ç›¸ä¼¼åº¦ \[J_{PPO}^{\theta&#39;}=J^{\theta&#39;}(\theta)-\beta\cdot KL(\theta, \theta&#39;)\] PPO2: Q Learning(Value-based) Learn Critic: è©•åƒ¹ actor \(\pi\) åšå¾—å¤šå¥½ï¼ˆè€Œä¸æ˜¯ stateï¼‰ State value function \(V^\pi(s)\) å®šç¾©ï¼šgiven sï¼Œåˆ°çµæŸçš„ç´¯ç©æœŸæœ› reward Estimate \(V^\pi(s)\) Monte-Carlo based çœ‹åˆ° state a å›å‚³ cumulated gain a ä¸å¯èƒ½æƒéæ‰€æœ‰ state \(\rightarrow V^\pi(s)\) è¦–ç‚º regression Temporal-difference appoarch \(\because s_t, a_t, r_t \rightarrow s_{t+1}, a_{t+1}, r_{t+1}\) MC vs. TD: ä¸åŒæ‰‹æ®µä¼°å‡ºä¾†çš„çµæœå¯èƒ½ä¸åŒ MC: larger varianceï¼Œå› ç‚ºæ¯æ¬¡å¾—åˆ°çš„ state &amp; gain æœ‰æŠ½æ¨£çš„éš¨æ©Ÿæ€§ï¼Œä¸”æ˜¯è¨ˆç®—ç´¯ç© reward TD: smaller varianceï¼Œå› ç‚ºæ¯æ¬¡åªè¨ˆç®—ä¸€å€‹ rï¼Œä½† \(V^\pi\) å¯èƒ½ä¼°çš„ä¸æº– State-action value function \(Q^\pi(s, a)\) å®šç¾©ï¼šåœ¨ state s å¼·åˆ¶æ¡å– action aï¼Œå¾ŒçºŒæ¡ç”¨ \(\pi\) çš„ç´¯ç©æœŸæœ› reward ï¼ˆagent ä¸ä¸€å®šæœƒæ¡ç”¨ aï¼‰ ä½¿ç”¨ Q function æ›´æ–° \(\pi\) çš„æµç¨‹ Tips: Target Network Exploration åŸå› ï¼šQ function æœ‰é»é¡ä¼¼ regressionï¼ˆç„¡éš¨æ©Ÿæ€§ï¼‰ï¼Œä¸é©åˆç”¨æ–¼ sample data è§£æ³•ï¼š Epsilon Greedy: Boltzmann Exploration Replay Buffer æ¸›å°‘å’Œç’°å¢ƒäº’å‹•çš„æ¬¡æ•¸ å¯æå‡ batch data diversity Off-policy: å› ç‚ºéå»çš„ exp ä¸ä¸€å®šä¾†è‡ª \(\pi\) Advanced Tips Double DQN å•é¡Œï¼šDQN æœƒé«˜ä¼° Q value ä¼°è¨ˆèª¤å·® \(\rightarrow\) é¸åˆ°é«˜ä¼°çš„ action \(\rightarrow\) targetå€¼æœƒè¢«è¨­å¤ªé«˜ è§£æ³•ï¼šé¸ action çš„ Q function \(\neq\) ç®— Q value çš„ Q function Dueling DQN æ”¹è®Š network æ¶æ§‹ï¼Œ\(Q(s, a)\rightarrow Q(s, a)=V(s)+A(s, a)\) æå‡ä¼°è¨ˆ Q value çš„æ•ˆç‡ï¼Œä¸ç”¨æ¯å€‹ pair éƒ½è¢« sample \(\because\) æ›´æ–° \(V(s)\) æ¯” sample æœ‰æ•ˆç‡ A çš„ column åˆç‚º 0ï¼Œç¢ºä¿æ›´æ–° V Priority Reply å¾ buffer è£¡é¸æ“‡ TD error è¼ƒå¤§çš„ experience set æ”¹è®Š sample data çš„ distribution &amp; training process" />
<link rel="canonical" href="https://jqlynchien713.github.io/2022/02/24/dqn.html" />
<meta property="og:url" content="https://jqlynchien713.github.io/2022/02/24/dqn.html" />
<meta property="og:site_name" content="Jacquelynâ€™s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-24T07:51:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="DQN å­¸ç¿’ç­†è¨˜" />
<script type="application/ld+json">
{"description":"é€™æ˜¯è«–æ–‡è¦ç ”ç©¶ã„‰ä¸»é¡Œï¼Œæƒ³èªªéƒ½å¯«å¥½äº†é‚£å°±å…ˆæ”¾ä¸Šä¾†å¥½äº† Basic Components Env, Actor, Reward Funciton, Episode &amp; Trajectory Trajectory: \\(\\tau = \\{s_i, a_i,\\dots\\}\\) in 1 episode å¯ä»¥è¨ˆç®—ç‰¹å®š (action, state) pair å‡ºç¾çš„æ©Ÿç‡\\(P_\\theta(\\tau) = p(s_1)p_\\theta(a_1\\|s_1)p(s_2\\|s_1, a_1)p_\\theta(a_2\\|s_2)p(s_3\\|s_2, a_2)\\dots\\\\=P(s_1)\\prod_{t=1}^{T}p_\\theta(a_t\\|s_t)p(s_{t+1}\\|s_t, a_t)\\) æ›´æ–° \\(\\theta\\) æ‰¾åˆ°æœ€å¤§çš„ reward: \\(\\max R(\\tau)=\\sum_t^Tr_t\\) Env &amp; Reward ç‚ºéš¨æ©Ÿè®Šæ•¸ï¼Œå› æ­¤åªèƒ½ç®—æœŸæœ›å€¼ï¼š \\(\\bar{R}_\\theta = \\sum R(\\tau)p_\\theta(\\tau) = E_{\\tau\\sim p_\\theta(\\tau)}[R(\\tau)]\\) Gradient Ascent: Maximize expected reward ç›®æ¨™ï¼šå¦‚æœæŸå€‹ (state, action) pair èƒ½è®“ trajectory reward ç‚ºæ­£ï¼Œå‰‡å¢åŠ é€™é …å‡ºç¾çš„æ©Ÿç‡ \\(\\nabla \\bar{R_\\theta}=\\frac{1}{N}\\sum_n^N\\sum_t^{T_n} R(\\tau^n)\\nabla\\log p_\\theta(a_t^n\\|s_t^n)\\) éœ€è¦ä¸æ–·å¾ trajectory set sample data Tips for implementation å°‡ state(input) &amp; action(output) è¦–ç‚ºåˆ†é¡å•é¡Œ ä¸€èˆ¬åˆ†é¡å•é¡Œ objective functionï¼šmin cross entropy = max log likelihood è½‰æ›æˆ RL æ™‚è¦å¤šä¹˜ä¸Š reward \\(\\frac{1}{N}\\sum_n^N\\sum_t^{T_n} \\nabla\\log p_\\theta(a_t^n\\|s_t^n)\\Rightarrow\\frac{1}{N}\\sum_n^N\\sum_t^{T_n} R(\\tau^n)\\nabla\\log p_\\theta(a_t^n\\|s_t^n)\\) Add a baseline æƒ…å¢ƒï¼šç•¶éŠæˆ²è¦å‰‡è£¡ä¸æœƒå¾—åˆ°è² åˆ†æ™‚ï¼›ä¸”æœ‰äº› action ä¸æœƒè¢« sample åˆ°ï¼Œå°è‡´ agent ä½ä¼°é€™å€‹action èƒ½å¸¶ä¾†çš„ reward è§£æ³•ï¼šè®“ reward ä¸è¦ç¸½æ˜¯æ­£çš„ \\(\\rightarrow\\) Reward - baseline(ex. \\(\\bar{\\tau^n}\\)) Assign Suitable Credit å•é¡Œï¼šå°±ç®—æŸå€‹ pair çš„è¡¨ç¾æ˜¯ä¸å¥½çš„ï¼Œä½†åœ¨ç¸½ reward ç‚ºæ­£çš„æƒ…æ³ä¸‹ï¼Œé‚„æ˜¯æœƒè¢«æå‡å‡ºç¾çš„æ©Ÿç‡ è§£æ³•ï¼šåªè¨ˆç®—åŸ·è¡Œæ­¤ action å¾Œçš„ reward æ–°çš„æ›´æ–°æ–¹å¼ï¼š \\(\\frac{1}{N}\\sum_n^N\\sum_t^{T_n} (R(\\tau^n)-b)\\nabla\\log p_\\theta(a_t^n\\|s_t^n)\\) b å–æ±ºæ–¼ state \\((R(\\tau^n)-b)\\) ç‚º advantage function \\(A^\\theta(s_t, a_t)\\) Proximal Policy Optimization, PPO Off-Policy On-policy vs. Off-policy On: agent å’Œç’°å¢ƒäº’å‹•ï¼‹å­¸ç¿’ Off: å­¸ç¿’çš„ agent \\(\\neq\\) äº’å‹•çš„ agentï¼Œaka agent åœ¨æ—é‚Šçœ‹åˆ¥äººç©ï¼ˆä¸¦å­¸ï¼‰ On-policy çš„å•é¡Œï¼šæ›´æ–° model å¾Œéœ€è¦é‡æ–° sample data Off-policy å¦‚ä½•è§£æ±ºï¼šsample \\(\\pi_{\\theta&#39;}\\) ä¾†è¨“ç·´ \\(\\pi_\\theta \\rightarrow\\) reuse sampled data Importance Sampling ç„¡æ³•ç›´æ¥å¾ p(x) æŠ½æ¨£çš„æƒ…æ³ä¸‹ï¼Œå¯è—‰ç”±å¦ä¸€å€‹åˆ†éƒ¨ q(x) æŠ½æ¨£ ä¸” sample q æ¬¡æ•¸å¤ å¤šå‰‡å¯è¶Šè¶¨è¿‘æ–¼ p(x) \\(E_{x\\sim p}[f(x)]=E_{x\\sim q}[f(x)\\frac{p(x)}{q(x)}]\\) \\(\\frac{p(x)}{q(x)}\\)ç‚ºä¿®æ­£å¾ q æŠ½æ¨£çš„èª¿æ•´é … Off-policy åš gradient ascent æ™‚çš„èª¿æ•´ \\(\\nabla \\bar{R_\\theta}=E_{\\tau\\sim p_\\theta(\\tau)}[R(\\tau)\\nabla\\log p_\\theta(\\tau)]\\rightarrow E_{\\tau\\sim p_{\\theta&#39;}(\\tau)}[\\frac{p_\\theta(\\tau)}{p_{\\theta&#39;}(\\tau)}R(\\tau)\\nabla\\log p_\\theta(\\tau)]\\) New Objective Function \\(J^{\\theta&#39;}(\\theta)=E_{(s_t,a_t)\\sim \\pi_{\\theta&#39;}(\\tau)}[\\frac{p_\\theta(a_t\\|s_t)}{p_{\\theta&#39;}(a_t\\|s_t)}A^{\\theta&#39;}(s_t, a_t)]\\) \\(\\theta&#39;\\): demo \\(\\theta\\): update PPO: é™åˆ¶ \\(\\pi\\) èˆ‡ \\(\\pi&#39;\\) é–“çš„ç›¸ä¼¼åº¦ \\[J_{PPO}^{\\theta&#39;}=J^{\\theta&#39;}(\\theta)-\\beta\\cdot KL(\\theta, \\theta&#39;)\\] PPO2: Q Learning(Value-based) Learn Critic: è©•åƒ¹ actor \\(\\pi\\) åšå¾—å¤šå¥½ï¼ˆè€Œä¸æ˜¯ stateï¼‰ State value function \\(V^\\pi(s)\\) å®šç¾©ï¼šgiven sï¼Œåˆ°çµæŸçš„ç´¯ç©æœŸæœ› reward Estimate \\(V^\\pi(s)\\) Monte-Carlo based çœ‹åˆ° state a å›å‚³ cumulated gain a ä¸å¯èƒ½æƒéæ‰€æœ‰ state \\(\\rightarrow V^\\pi(s)\\) è¦–ç‚º regression Temporal-difference appoarch \\(\\because s_t, a_t, r_t \\rightarrow s_{t+1}, a_{t+1}, r_{t+1}\\) MC vs. TD: ä¸åŒæ‰‹æ®µä¼°å‡ºä¾†çš„çµæœå¯èƒ½ä¸åŒ MC: larger varianceï¼Œå› ç‚ºæ¯æ¬¡å¾—åˆ°çš„ state &amp; gain æœ‰æŠ½æ¨£çš„éš¨æ©Ÿæ€§ï¼Œä¸”æ˜¯è¨ˆç®—ç´¯ç© reward TD: smaller varianceï¼Œå› ç‚ºæ¯æ¬¡åªè¨ˆç®—ä¸€å€‹ rï¼Œä½† \\(V^\\pi\\) å¯èƒ½ä¼°çš„ä¸æº– State-action value function \\(Q^\\pi(s, a)\\) å®šç¾©ï¼šåœ¨ state s å¼·åˆ¶æ¡å– action aï¼Œå¾ŒçºŒæ¡ç”¨ \\(\\pi\\) çš„ç´¯ç©æœŸæœ› reward ï¼ˆagent ä¸ä¸€å®šæœƒæ¡ç”¨ aï¼‰ ä½¿ç”¨ Q function æ›´æ–° \\(\\pi\\) çš„æµç¨‹ Tips: Target Network Exploration åŸå› ï¼šQ function æœ‰é»é¡ä¼¼ regressionï¼ˆç„¡éš¨æ©Ÿæ€§ï¼‰ï¼Œä¸é©åˆç”¨æ–¼ sample data è§£æ³•ï¼š Epsilon Greedy: Boltzmann Exploration Replay Buffer æ¸›å°‘å’Œç’°å¢ƒäº’å‹•çš„æ¬¡æ•¸ å¯æå‡ batch data diversity Off-policy: å› ç‚ºéå»çš„ exp ä¸ä¸€å®šä¾†è‡ª \\(\\pi\\) Advanced Tips Double DQN å•é¡Œï¼šDQN æœƒé«˜ä¼° Q value ä¼°è¨ˆèª¤å·® \\(\\rightarrow\\) é¸åˆ°é«˜ä¼°çš„ action \\(\\rightarrow\\) targetå€¼æœƒè¢«è¨­å¤ªé«˜ è§£æ³•ï¼šé¸ action çš„ Q function \\(\\neq\\) ç®— Q value çš„ Q function Dueling DQN æ”¹è®Š network æ¶æ§‹ï¼Œ\\(Q(s, a)\\rightarrow Q(s, a)=V(s)+A(s, a)\\) æå‡ä¼°è¨ˆ Q value çš„æ•ˆç‡ï¼Œä¸ç”¨æ¯å€‹ pair éƒ½è¢« sample \\(\\because\\) æ›´æ–° \\(V(s)\\) æ¯” sample æœ‰æ•ˆç‡ A çš„ column åˆç‚º 0ï¼Œç¢ºä¿æ›´æ–° V Priority Reply å¾ buffer è£¡é¸æ“‡ TD error è¼ƒå¤§çš„ experience set æ”¹è®Š sample data çš„ distribution &amp; training process","url":"https://jqlynchien713.github.io/2022/02/24/dqn.html","headline":"DQN å­¸ç¿’ç­†è¨˜","@type":"BlogPosting","dateModified":"2022-02-24T07:51:00+00:00","datePublished":"2022-02-24T07:51:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://jqlynchien713.github.io/2022/02/24/dqn.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css">
  <link rel="shortcut icon" type="image/png" href="/favicon.png"><link type="application/atom+xml" rel="alternate" href="https://jqlynchien713.github.io/feed.xml" title="Jacquelyn's Blog" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Jacquelyn&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/tags/">Tags</a><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <!-- enable latex -->
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">DQN å­¸ç¿’ç­†è¨˜</h1>
    <p class="post-meta">
    <time class="dt-published" datetime="2022-02-24T07:51:00+00:00" itemprop="datePublished">Feb 24, 2022
      | ğŸ‘€
      <span class="reading-time" title="Estimated read time">
        
        
                                 10 mins
                                 
      </span>
    </time>
    |
    
      <span class="tag">update</span>
    
      <span class="tag">research</span>
    </p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <div class="sidebar"><ul><li><a href="#basic-components">Basic Components</a></li><li><a href="#gradient-ascent-maximize-expected-reward">Gradient Ascent: Maximize expected reward</a></li><li><a href="#proximal-policy-optimization-ppo">Proximal Policy Optimization, PPO</a></li><li><a href="#q-learningvalue-based">Q Learning(Value-based)</a></li></ul></div>
    <blockquote>
  <p>é€™æ˜¯è«–æ–‡è¦ç ”ç©¶ã„‰ä¸»é¡Œï¼Œæƒ³èªªéƒ½å¯«å¥½äº†é‚£å°±å…ˆæ”¾ä¸Šä¾†å¥½äº†</p>
</blockquote>

<h2 id="basic-components">Basic Components</h2>
<p><img src="https://i.imgur.com/Ht2mciG.png" width="300" /></p>

<ul>
  <li>Env, Actor, Reward Funciton, Episode &amp; Trajectory</li>
  <li>Trajectory: \(\tau = \{s_i, a_i,\dots\}\) in 1 episode</li>
  <li>å¯ä»¥è¨ˆç®—ç‰¹å®š (action, state) pair å‡ºç¾çš„æ©Ÿç‡\(P_\theta(\tau) = p(s_1)p_\theta(a_1\|s_1)p(s_2\|s_1, a_1)p_\theta(a_2\|s_2)p(s_3\|s_2, a_2)\dots\\=P(s_1)\prod_{t=1}^{T}p_\theta(a_t\|s_t)p(s_{t+1}\|s_t, a_t)\)</li>
  <li>æ›´æ–° \(\theta\) æ‰¾åˆ°æœ€å¤§çš„ reward: \(\max R(\tau)=\sum_t^Tr_t\)
    <ul>
      <li>Env &amp; Reward ç‚ºéš¨æ©Ÿè®Šæ•¸ï¼Œå› æ­¤åªèƒ½ç®—æœŸæœ›å€¼ï¼š \(\bar{R}_\theta = \sum R(\tau)p_\theta(\tau) = E_{\tau\sim p_\theta(\tau)}[R(\tau)]\)</li>
    </ul>
  </li>
</ul>

<h2 id="gradient-ascent-maximize-expected-reward">Gradient Ascent: Maximize expected reward</h2>
<ul>
  <li>ç›®æ¨™ï¼šå¦‚æœæŸå€‹ (state, action) pair èƒ½è®“ trajectory reward ç‚ºæ­£ï¼Œå‰‡å¢åŠ é€™é …å‡ºç¾çš„æ©Ÿç‡
   \(\nabla \bar{R_\theta}=\frac{1}{N}\sum_n^N\sum_t^{T_n} R(\tau^n)\nabla\log p_\theta(a_t^n\|s_t^n)\)</li>
  <li>éœ€è¦ä¸æ–·å¾ trajectory set sample data
<img src="https://i.imgur.com/Q6sCVDq.png" alt="" /></li>
  <li>Tips for implementation
    <ul>
      <li>å°‡ state(input) &amp; action(output) è¦–ç‚ºåˆ†é¡å•é¡Œ
        <ul>
          <li>ä¸€èˆ¬åˆ†é¡å•é¡Œ objective functionï¼šmin cross entropy = max log likelihood</li>
          <li>è½‰æ›æˆ RL æ™‚è¦å¤šä¹˜ä¸Š reward
  \(\frac{1}{N}\sum_n^N\sum_t^{T_n} \nabla\log p_\theta(a_t^n\|s_t^n)\Rightarrow\frac{1}{N}\sum_n^N\sum_t^{T_n} R(\tau^n)\nabla\log p_\theta(a_t^n\|s_t^n)\)</li>
        </ul>
      </li>
      <li>Add a baseline
        <ul>
          <li>æƒ…å¢ƒï¼šç•¶éŠæˆ²è¦å‰‡è£¡ä¸æœƒå¾—åˆ°è² åˆ†æ™‚ï¼›ä¸”æœ‰äº› action ä¸æœƒè¢« sample åˆ°ï¼Œå°è‡´ agent ä½ä¼°é€™å€‹action èƒ½å¸¶ä¾†çš„ reward</li>
          <li>è§£æ³•ï¼šè®“ reward ä¸è¦ç¸½æ˜¯æ­£çš„ \(\rightarrow\) Reward - baseline(ex. \(\bar{\tau^n}\))</li>
        </ul>
      </li>
      <li>Assign Suitable Credit
        <ul>
          <li>å•é¡Œï¼šå°±ç®—æŸå€‹ pair çš„è¡¨ç¾æ˜¯ä¸å¥½çš„ï¼Œä½†åœ¨ç¸½ reward ç‚ºæ­£çš„æƒ…æ³ä¸‹ï¼Œé‚„æ˜¯æœƒè¢«æå‡å‡ºç¾çš„æ©Ÿç‡</li>
          <li>è§£æ³•ï¼šåªè¨ˆç®—åŸ·è¡Œæ­¤ action å¾Œçš„ reward
<img src="https://i.imgur.com/8J18po0.png" alt="" /></li>
          <li>æ–°çš„æ›´æ–°æ–¹å¼ï¼š
  \(\frac{1}{N}\sum_n^N\sum_t^{T_n} (R(\tau^n)-b)\nabla\log p_\theta(a_t^n\|s_t^n)\)
            <ul>
              <li>b å–æ±ºæ–¼ state</li>
              <li>\((R(\tau^n)-b)\) ç‚º advantage function \(A^\theta(s_t, a_t)\)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="proximal-policy-optimization-ppo">Proximal Policy Optimization, PPO</h2>
<ul>
  <li>Off-Policy
    <ul>
      <li>On-policy vs. Off-policy
        <ul>
          <li>On: agent å’Œç’°å¢ƒäº’å‹•ï¼‹å­¸ç¿’</li>
          <li>Off: å­¸ç¿’çš„ agent \(\neq\) äº’å‹•çš„ agentï¼Œaka agent åœ¨æ—é‚Šçœ‹åˆ¥äººç©ï¼ˆä¸¦å­¸ï¼‰</li>
        </ul>
      </li>
      <li>On-policy çš„å•é¡Œï¼šæ›´æ–° model å¾Œéœ€è¦é‡æ–° sample data</li>
      <li>
        <p>Off-policy å¦‚ä½•è§£æ±ºï¼šsample \(\pi_{\theta'}\) ä¾†è¨“ç·´ \(\pi_\theta \rightarrow\) reuse sampled data</p>

        <blockquote>
          <p><strong>Importance Sampling</strong> <br />
ç„¡æ³•ç›´æ¥å¾ p(x) æŠ½æ¨£çš„æƒ…æ³ä¸‹ï¼Œå¯è—‰ç”±å¦ä¸€å€‹åˆ†éƒ¨ q(x) æŠ½æ¨£
ä¸” sample q æ¬¡æ•¸å¤ å¤šå‰‡å¯è¶Šè¶¨è¿‘æ–¼ p(x)
\(E_{x\sim p}[f(x)]=E_{x\sim q}[f(x)\frac{p(x)}{q(x)}]\)
\(\frac{p(x)}{q(x)}\)ç‚ºä¿®æ­£å¾ q æŠ½æ¨£çš„èª¿æ•´é …</p>
        </blockquote>
      </li>
      <li>Off-policy åš gradient ascent æ™‚çš„èª¿æ•´
  \(\nabla \bar{R_\theta}=E_{\tau\sim p_\theta(\tau)}[R(\tau)\nabla\log p_\theta(\tau)]\rightarrow E_{\tau\sim p_{\theta'}(\tau)}[\frac{p_\theta(\tau)}{p_{\theta'}(\tau)}R(\tau)\nabla\log p_\theta(\tau)]\)</li>
      <li>New Objective Function
  \(J^{\theta'}(\theta)=E_{(s_t,a_t)\sim \pi_{\theta'}(\tau)}[\frac{p_\theta(a_t\|s_t)}{p_{\theta'}(a_t\|s_t)}A^{\theta'}(s_t, a_t)]\)
        <ul>
          <li>\(\theta'\): demo</li>
          <li>\(\theta\): update</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>PPO: é™åˆ¶ \(\pi\) èˆ‡ \(\pi'\) é–“çš„ç›¸ä¼¼åº¦
    <ul>
      <li>
\[J_{PPO}^{\theta'}=J^{\theta'}(\theta)-\beta\cdot KL(\theta, \theta')\]
      </li>
    </ul>
  </li>
  <li>PPO2:</li>
</ul>

<h2 id="q-learningvalue-based">Q Learning(Value-based)</h2>
<p>Learn Critic: è©•åƒ¹ <strong>actor \(\pi\)</strong> åšå¾—å¤šå¥½ï¼ˆè€Œä¸æ˜¯ stateï¼‰</p>
<ul>
  <li>State value function \(V^\pi(s)\)
    <ul>
      <li>å®šç¾©ï¼šgiven sï¼Œåˆ°çµæŸçš„ç´¯ç©<strong>æœŸæœ›</strong> reward</li>
      <li>Estimate \(V^\pi(s)\)
        <ul>
          <li>Monte-Carlo based
            <ul>
              <li>çœ‹åˆ° state a å›å‚³ cumulated gain a</li>
              <li>ä¸å¯èƒ½æƒéæ‰€æœ‰ state \(\rightarrow V^\pi(s)\) è¦–ç‚º regression</li>
            </ul>
          </li>
          <li>Temporal-difference appoarch
\(\because s_t, a_t, r_t \rightarrow s_{t+1}, a_{t+1}, r_{t+1}\)
<img src="https://i.imgur.com/DiOr0kj.png" alt="" /></li>
          <li>MC vs. TD: ä¸åŒæ‰‹æ®µä¼°å‡ºä¾†çš„çµæœå¯èƒ½ä¸åŒ
            <ul>
              <li>MC: larger varianceï¼Œå› ç‚ºæ¯æ¬¡å¾—åˆ°çš„ state &amp; gain æœ‰æŠ½æ¨£çš„éš¨æ©Ÿæ€§ï¼Œä¸”æ˜¯è¨ˆç®—ç´¯ç© reward</li>
              <li>TD: smaller varianceï¼Œå› ç‚ºæ¯æ¬¡åªè¨ˆç®—ä¸€å€‹ rï¼Œä½† \(V^\pi\) å¯èƒ½ä¼°çš„ä¸æº–</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>State-action value function \(Q^\pi(s, a)\)
    <ul>
      <li>å®šç¾©ï¼š<span style="background: #FFFF33">åœ¨ state s <strong>å¼·åˆ¶æ¡å– action a</strong>ï¼Œå¾ŒçºŒæ¡ç”¨ \(\pi\) çš„ç´¯ç©æœŸæœ› reward</span>
  ï¼ˆagent ä¸ä¸€å®šæœƒæ¡ç”¨ aï¼‰
  <img src="https://i.imgur.com/ONuC5sa.png" alt="" /></li>
      <li>ä½¿ç”¨ Q function æ›´æ–° \(\pi\) çš„æµç¨‹
  <img src="https://i.imgur.com/VkljFxs.png" alt="" /></li>
    </ul>
  </li>
  <li>Tips:
    <ul>
      <li>Target Network
   <img src="https://i.imgur.com/bbimeav.png" alt="" /></li>
      <li>Exploration
        <ul>
          <li>åŸå› ï¼šQ function æœ‰é»é¡ä¼¼ regressionï¼ˆç„¡éš¨æ©Ÿæ€§ï¼‰ï¼Œä¸é©åˆç”¨æ–¼ sample data</li>
          <li>è§£æ³•ï¼š
            <ul>
              <li>Epsilon Greedy: <img src="https://i.imgur.com/dzvQtq3.png" alt="" /></li>
              <li>Boltzmann Exploration</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Replay Buffer
  <img src="https://i.imgur.com/QLBWaPY.jpg" alt="" />
        <ul>
          <li>æ¸›å°‘å’Œç’°å¢ƒäº’å‹•çš„æ¬¡æ•¸</li>
          <li>å¯æå‡ batch data diversity</li>
          <li>Off-policy: å› ç‚ºéå»çš„ exp ä¸ä¸€å®šä¾†è‡ª \(\pi\)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Advanced Tips
    <ul>
      <li>Double DQN
        <ul>
          <li>å•é¡Œï¼šDQN æœƒé«˜ä¼° Q value
            <ul>
              <li>ä¼°è¨ˆèª¤å·® \(\rightarrow\) é¸åˆ°é«˜ä¼°çš„ action \(\rightarrow\) targetå€¼æœƒè¢«è¨­å¤ªé«˜</li>
            </ul>
          </li>
          <li>è§£æ³•ï¼šé¸ action çš„ Q function \(\neq\) ç®— Q value çš„ Q function
            <ul>
              <li><img src="https://i.imgur.com/7KlxIx9.png" alt="" /></li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Dueling DQN
        <ul>
          <li>æ”¹è®Š network æ¶æ§‹ï¼Œ\(Q(s, a)\rightarrow Q(s, a)=V(s)+A(s, a)\)</li>
          <li>æå‡ä¼°è¨ˆ Q value çš„æ•ˆç‡ï¼Œä¸ç”¨æ¯å€‹ pair éƒ½è¢« sample</li>
          <li><img src="https://i.imgur.com/z1WzTyg.jpg" alt="" /></li>
          <li>\(\because\) æ›´æ–° \(V(s)\) æ¯” sample æœ‰æ•ˆç‡</li>
          <li>A çš„ column åˆç‚º 0ï¼Œç¢ºä¿æ›´æ–° V</li>
        </ul>
      </li>
      <li>Priority Reply
        <ul>
          <li>å¾ buffer è£¡é¸æ“‡ TD error è¼ƒå¤§çš„ experience set</li>
          <li>æ”¹è®Š sample data çš„ distribution &amp; training process</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

  </div>

  
  
    <script src="https://utteranc.es/client.js"
            repo=jqlynchien713/jqlynchien713.github.io
            issue-term=pathname
            label=Comments
            theme=github-light
            crossorigin= "anonymous"
            async>
    </script>
  

<a class="u-url" href="/2022/02/24/dqn.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Jacquelyn&#39;s Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Jacquelyn&#39;s Blog</li><li><a class="u-email" href="mailto:jqlynchien713@gmail.com">jqlynchien713@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-3"><ul class="social-media-list"><li><a href="https://github.com/jqlynchien713"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jqlynchien713</span></a></li></ul>
</div>

      <div class="footer-col footer-col-2">
        <p>Ê• â€¢á´¥â€¢Ê”</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
